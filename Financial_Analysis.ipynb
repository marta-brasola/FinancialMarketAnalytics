{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marta-brasola/FinancialMarketAnalytics/blob/main/Financial_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjWrscoRWk1d"
      },
      "source": [
        "# Financial Market Analytics\n",
        "Marta Brasola, Luca Sammarini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6FwWNhEyOBL9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pprint import pprint\n",
        "from tabulate import tabulate\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f7OhFfz4Wk1o"
      },
      "outputs": [],
      "source": [
        "custom_params = {\n",
        "    \"axes.spines.right\": False,\n",
        "    \"axes.spines.top\": False,\n",
        "    \"axes.grid\": True,         # Enable grid\n",
        "    \"grid.color\": \"gray\",  # Set grid color to light gray\n",
        "    \"grid.linestyle\": \"--\",     # Set grid line style to dashed\n",
        "    \"grid.linewidth\": 0.5,      # Set grid line width\n",
        "    \"figure.facecolor\": \"#fbfbfb\",\n",
        "    \"axes.facecolor\": \"#fbfbfb\"\n",
        "}\n",
        "\n",
        "sns.set_theme(style=\"ticks\", rc=custom_params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Y7alaDW9GjkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ibc8olBvOETK"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/financial/data/Euro_preprocessed.csv', header=[0,1], index_col=[0])\n",
        "df = df.replace(0, np.nan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUEGj6wOWk1q"
      },
      "source": [
        "## Assumptions\n",
        "\n",
        "- azioni frazionabili all'infinito\n",
        "- no costi di commission (ma abbiamo fatto un'analisi di turnover)\n",
        "- capitale infinito\n",
        "- portafoglio equally wighted\n",
        "- shift degli fundamental factors che sono ricavati da dati aziendali di bilancio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hYvqSTrTz-p"
      },
      "outputs": [],
      "source": [
        "def get_log_returns(data):\n",
        "  \"\"\"\n",
        "  function to get log returns from a dataframe\n",
        "  \"\"\"\n",
        "  prices = data['PX_LAST'].reset_index()\n",
        "  prices = prices.rename_axis(None, axis=1)\n",
        "  prices = prices.replace(0, np.NAN)\n",
        "  prices = prices.fillna(np.nan)\n",
        "  prices = prices.set_index('Date')\n",
        "  returns = np.log(prices/prices.shift(1))\n",
        "  returns = returns.iloc[1::]\n",
        "  return returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRL8ZkGrWk1s"
      },
      "outputs": [],
      "source": [
        "returns = get_log_returns(df)\n",
        "\n",
        "indicators = df.columns.get_level_values(0).unique()\n",
        "\n",
        "benchmark_returns = returns.mean(axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUfYFpf4Tz-w"
      },
      "source": [
        "## Checking missing data\n",
        "\n",
        "When conducting financial analysis, it is crucial to examine the presence of missing values in a dataset, as they can significantly impact the accuracy and reliability of the analysis. Missing values can arise due to various reasons such as data collection errors, system issues, or intentional non-disclosure.\n",
        "\n",
        "One important consideration in financial analysis is the potential for survivor bias. Survivor bias occurs when only the data of entities that have \"survived\" or persisted until the present time are included in the analysis, while the data of entities that did not survive or were excluded are omitted. In a financial context, survivor bias can distort the analysis by providing an overly optimistic view of performance, as it neglects the experiences of entities that may have faced adverse outcomes.\n",
        "\n",
        "To address this concern, it is essential to carefully examine and handle missing values in a comprehensive manner. This may involve imputing missing data using appropriate techniques, considering the reasons for missingness, or conducting sensitivity analyses to assess the impact of missing values on the overall results. By acknowledging and addressing missing values, financial analysts can enhance the robustness and accuracy of their analyses, avoiding potential pitfalls associated with survivor bias.\n",
        "\n",
        "#### missing data for each factor\n",
        "\n",
        "To check which factors have most null values we computed the number of null values over the total possible values, which is 111 months times 797 total stocks. We than performed a filter action to exclude those factors that have more than 50% of missing data in the entire stock universe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DB6dR53CTz-x"
      },
      "outputs": [],
      "source": [
        "tot_values = 111*797\n",
        "null_ind = df.isnull().groupby(level=0, axis=1).sum().sum(axis=0).reset_index()\n",
        "null_ind.columns = ['indicators', 'null_values']\n",
        "null_ind['perc_null_values'] = (null_ind['null_values'] / tot_values) * 100\n",
        "null_ind.sort_values(by='perc_null_values', ascending=False)\n",
        "ind_drop = list(null_ind[null_ind['perc_null_values']>50]['indicators'])\n",
        "print(f\"indicators to drop:\\n\")\n",
        "pprint(ind_drop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIJVyY9y4UXt"
      },
      "outputs": [],
      "source": [
        "df.drop(columns=ind_drop, inplace=True)\n",
        "prova = len(df.columns.get_level_values(0).unique())\n",
        "print(f\"number of indicators remaining: {prova}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Xy8Bfk2inSE"
      },
      "source": [
        "### missing data analysis over time\n",
        "\n",
        "After performing the analysis of missing values over the entire dataframe we wanted to see how null values distribute over time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnWP-l5GiaTF"
      },
      "outputs": [],
      "source": [
        "indicators = df.columns.get_level_values(0).unique()\n",
        "\n",
        "# create dataframe to plot the data\n",
        "null = {}\n",
        "\n",
        "for ind in indicators:\n",
        "  null[ind] = (\n",
        "    df.stack().reset_index().drop('tickers',axis=1)\n",
        "    .groupby('Date')[ind]\n",
        "    .apply(lambda x: x.isnull().sum())\n",
        "  )\n",
        "\n",
        "null = pd.DataFrame.from_dict(null)\n",
        "null = round(null / 797, 2) * 100\n",
        "\n",
        "labels = df.index[::3]\n",
        "\n",
        "plt.figure(figsize=(15,6))\n",
        "\n",
        "# Plot the data\n",
        "plt.plot(null)\n",
        "\n",
        "plt.xticks(rotation=90, ha='center', labels=labels, ticks=range(0, len(df.index), 3))\n",
        "plt.xticks(rotation=45, ha='right', labels=['']*len(df.index), ticks=range(len(df.index)), minor=True)\n",
        "\n",
        "threshold = 80\n",
        "indicators_with_high_nulls = null.columns[null.mean() > threshold / 100]\n",
        "for indicator in indicators_with_high_nulls:\n",
        "    plt.plot([], label=indicator)  # Empty plot to create space for legend\n",
        "\n",
        "plt.legend(title='Indicators with >{}% Nulls'.format(threshold), loc='upper left', bbox_to_anchor=(1, 1))\n",
        "\n",
        "plt.title('Percentage of missing values for each indicator over time', fontsize=16)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nETqylRvWk10"
      },
      "source": [
        "At the beginning the are many null values for most of the factors that where given but at the end we decided not drop any more data to avoid introducing any more biases into our analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSAq7SnpWk10"
      },
      "source": [
        "## Preprocessing\n",
        "In this part we created a few dataframe with different structures that will be useful later on to perform our analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DbJCmUlWk11"
      },
      "outputs": [],
      "source": [
        "df2 = df.copy(deep=True)\n",
        "df2.drop(df2.head(1).index, inplace=True)\n",
        "\n",
        "r = pd.DataFrame(returns.values, index=returns.index,\n",
        "                 columns=pd.MultiIndex.from_product([['RETURNS'],\n",
        "                                                     returns.columns]))\n",
        "\n",
        "df2 = pd.concat([df2, r], axis=1)\n",
        "data_stack = df2.stack()\n",
        "data_stack.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEztah9hiy5Q"
      },
      "source": [
        "## Univariate Ranking\n",
        "\n",
        "Univariate stock ranking is a method of evaluating and ranking individual stocks based on a single, specific criterion or factor. Instead of considering multiple factors simultaneously, univariate stock ranking focuses on a single variable to assess the relative strength or performance of different stocks.\n",
        "\n",
        "Univariate stock ranking simplifies the analysis by focusing on a single factor, allowing investors or analysts to quickly identify and compare stocks based on that specific criterion. However, it's important to note that a comprehensive stock analysis often involves considering multiple factors to gain a more holistic view of a stock's potential. Many investors combine univariate ranking with multivariate approaches for a more nuanced evaluation of investment opportunities.\n",
        "\n",
        "\n",
        "Look-ahead bias refers to a situation in data analysis or financial modeling where information that would not have been known or available at a certain point in time is improperly used to make predictions or decisions for that past period. This bias occurs when future data or information is inadvertently included in historical datasets, leading to an inaccurate representation of what information was actually available at the time.\n",
        "\n",
        "In financial analysis, look-ahead bias can distort the evaluation of investment strategies, backtesting results, or performance metrics. It may lead to overestimating the effectiveness of a strategy because it mistakenly incorporates information that was not available at the time the decisions were made.\n",
        "\n",
        "First, we compiled a list of factors derived from balance sheet data. In our historical dataset, we applied lagging to account for reporting and release delays. A common strategy to address look-ahead bias involves understanding the delivery lag associated with the factors being utilized and incorporating an appropriate lag. For instance, if a factor is related to March but is only reported in April, it is advisable to use the t − 1 (previous time period) value of that factor in any historical estimations. This concept is elaborated upon in greater detail in the following discussion.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vZ_6VBDWk13"
      },
      "outputs": [],
      "source": [
        "financial_ratios = [\n",
        "    \"PE_RATIO\",\n",
        "    \"FIVE_YR_AVG_PRICE_EARNINGS\",\n",
        "    \"T12M_DIL_PE_CONT_OPS\",\n",
        "    \"10_YEAR_MOVING_AVERAGE_PE\",\n",
        "    \"PX_TO_TANG_BV_PER_SH\",\n",
        "    \"CURRENT_EV_TO_12M_SALES\",\n",
        "    \"CURRENT_EV_TO_T12M_EBITDA\",\n",
        "    \"FIVE_YEAR_AVG_EV_TO_T12_EBITDA\",\n",
        "    \"T12M_DIL_EPS_CONT_OPS\",\n",
        "    \"TRAIL_12M_EBITDA_PER_SHARE\",\n",
        "    \"TRAIL_12M_SALES_PER_SH\",\n",
        "    \"NET_DEBT_PER_SHARE\",\n",
        "    \"TANG_BOOK_VAL_PER_SH\",\n",
        "    \"NORMALIZED_ACCRUALS_CF_METHOD\",\n",
        "    \"EBITDA_MARGIN\",\n",
        "    \"EBITDA_MARGIN_3YR_AVG\",\n",
        "    \"CAP_EXPEND_TO_SALES\",\n",
        "    \"T12M_DVD_PAYOUT_RATIO\",\n",
        "    \"EQY_DPS_NET_5YR_GROWTH\",\n",
        "    \"NORMALIZED_ROE\",\n",
        "    \"5YR_AVG_RETURN_ON_EQUITY\",\n",
        "    \"NORMALIZED_ACCRUALS_BS_METHOD\",\n",
        "    \"PX_TO_BOOK_RATIO\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2APU5IwpWk13"
      },
      "source": [
        "The provided Python functions offer a valuable toolset for quantitative financial analysis. The `calculate_quantile_returns` function takes a DataFrame containing stock-specific factor values, divides them into quantiles, and computes returns for each quantile. This facilitates the assessment of how a specified factor influences stock returns. Additionally, the `calculate_IR` function calculates the Information Ratio (IR), a metric that measures the excess return of a portfolio relative to a benchmark, considering the associated tracking error. These functions are useful for evaluating and comparing the performance of investment strategies based on specific factors, aiding in the identification of potential alpha-generating opportunities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAlLnPcqWk14"
      },
      "outputs": [],
      "source": [
        "def calculate_quantile_returns(data_frame, column_name, returns_df, num_quantiles=5):\n",
        "\n",
        "    \"\"\"\n",
        "    Calculate quantile returns based on the specified factor in the DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    quantile_ranks = pd.DataFrame(index=data_frame.index, columns=data_frame.columns.levels[1])\n",
        "    ind_rank = data_frame[column_name]\n",
        "    ind_rank = ind_rank.replace(0, np.nan)\n",
        "\n",
        "    # ranking stock for every date based on the values of the month\n",
        "    # before (if we consider a fundamental factor)\n",
        "    for date in ind_rank.index:\n",
        "\n",
        "        if column_name in financial_ratios:\n",
        "            row_values = ind_rank.shift(1).loc[date]\n",
        "        else:\n",
        "            row_values = ind_rank.loc[date]\n",
        "\n",
        "        if row_values.count() > 1:\n",
        "            ranks = pd.Series(row_values).rank(method='max')\n",
        "            quintiles = pd.qcut(ranks, q=num_quantiles, labels=False)\n",
        "            quantile_ranks.loc[date] = quintiles\n",
        "\n",
        "    # calculate returns for each quantile\n",
        "    quantile_dfs = {}\n",
        "    portfolio_returns = pd.DataFrame()\n",
        "\n",
        "    for quantile in range(num_quantiles):\n",
        "        filtered_df = returns_df[quantile_ranks == quantile]\n",
        "        filtered_df_shifted = filtered_df\n",
        "        quantile_dfs[quantile] = filtered_df_shifted\n",
        "        portfolio_returns[quantile] = quantile_dfs[quantile].mean(axis=1).dropna()\n",
        "\n",
        "    return quantile_dfs, portfolio_returns\n",
        "\n",
        "\n",
        "def calculate_IR(portfolio_returns, benchmark_returns):\n",
        "    \"\"\"\n",
        "    Calculate the Information Ratio.\n",
        "    \"\"\"\n",
        "\n",
        "    active_return = portfolio_returns-benchmark_returns\n",
        "    tracking_error = np.std(portfolio_returns-benchmark_returns)\n",
        "\n",
        "\n",
        "    return active_return.mean() / tracking_error # ci va o meno l'annualizzazione?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLgAFZU9Wk15"
      },
      "source": [
        "### Evaluate the performances\n",
        "\n",
        "#### Information Ratio (IR) in Financial Analysis\n",
        "\n",
        "The Information Ratio (IR) is a widely-used metric in financial analysis that assesses the risk-adjusted performance of an investment strategy or portfolio compared to a benchmark. It provides insight into the excess return generated per unit of tracking error, offering a measure of the portfolio manager's skill in generating returns beyond the benchmark.\n",
        "\n",
        "**Calculation:**\n",
        "\n",
        "The Information Ratio is calculated using the following formula:\n",
        "\n",
        "$$\\text{IR} = \\frac{\\text{Active Return}}{\\text{Tracking Error}}$$\n",
        "\n",
        "Where:\n",
        "- **Active Return** is the portfolio return minus the benchmark return.\n",
        "- **Tracking Error** is the standard deviation of the active return.\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "- A positive Information Ratio indicates that the portfolio has outperformed the benchmark on a risk-adjusted basis.\n",
        "- A higher IR suggests a better risk-adjusted performance.\n",
        "- A negative IR implies underperformance relative to the benchmark.\n",
        "\n",
        "#### Benchmark\n",
        "\n",
        "As for the benchmark we decided to use a portfolio built with all the stock in the stock universe to test whether is worth it or not to select stock based our strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWdfpNybiC12"
      },
      "outputs": [],
      "source": [
        "# indicators = df.columns.get_level_values(0).unique()\n",
        "\n",
        "indicator_IR = pd.DataFrame(index=indicators, columns=['IR_top_quantile', 'IR_bottom_quantile', 'tracking_top_quantile',\n",
        "                                                       'tracking_error_bottom_quantile', 'returns_top_quantile', 'returns_bottom_quantile'])\n",
        "\n",
        "for indicator in indicators:\n",
        "    result_dfs, portfolio_returns = calculate_quantile_returns(df, indicator, returns)\n",
        "    indicator_IR.at[indicator, 'IR_top_quantile'] = calculate_IR(portfolio_returns[4], benchmark_returns)\n",
        "    indicator_IR.at[indicator, 'IR_bottom_quantile'] = calculate_IR(portfolio_returns[0], benchmark_returns)\n",
        "    indicator_IR.at[indicator, 'tracking_top_quantile'] = np.std(portfolio_returns[4]-benchmark_returns)\n",
        "    indicator_IR.at[indicator, 'tracking_error_bottom_quantile'] = np.std(portfolio_returns[0]-benchmark_returns)\n",
        "    indicator_IR.at[indicator, 'returns_top_quantile'] = np.mean(portfolio_returns[4]) * 100\n",
        "    indicator_IR.at[indicator, 'returns_bottom_quantile'] = np.mean(portfolio_returns[0]) * 100\n",
        "\n",
        "\n",
        "indicator_IR.sort_values('IR_top_quantile', ascending = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-UspUoQWk16"
      },
      "outputs": [],
      "source": [
        "selected_rows = (\n",
        "    (indicator_IR['IR_top_quantile'] > 0.50) |\n",
        "    (indicator_IR['IR_bottom_quantile'] > 0.50)\n",
        ")\n",
        "\n",
        "indicator_IR[selected_rows]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_NX5SLSWk17"
      },
      "source": [
        "- plot the returns of the selected rows over time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdttbkqWWk17"
      },
      "outputs": [],
      "source": [
        "# Selecting columns for plotting\n",
        "selected_columns = ['IR_top_quantile', 'IR_bottom_quantile']\n",
        "\n",
        "labels = ['CURRENT_EV_TO\\nT12M_EBITDA', 'EQY_REC_CONS',\n",
        "       'FIVE_YEAR_AVG\\nEV_TO_T12_EBITDA', 'PE_RATIO', 'RSI_14D', 'RSI_30D',\n",
        "       'RSI_9D']\n",
        "\n",
        "# Plotting the selected data\n",
        "ax = indicator_IR[selected_rows][selected_columns].plot(kind='bar', figsize=(10, 6))\n",
        "\n",
        "Xstart, Xend = ax.get_xlim()\n",
        "Ystart, Yend = ax.get_ylim()\n",
        "\n",
        "ax.text(Xstart+0.5, Yend+0.2, '''IR Comparison Between Top and Bottom Quantiles of indicators with IR > 0.5''', fontsize=14)\n",
        "\n",
        "plt.xlabel('indicators', fontsize=14)\n",
        "plt.ylabel('Informatio Ratio', fontsize=14)\n",
        "plt.legend(['First Quantile', 'Fifth Quantile'], fontsize=12)\n",
        "ax.set_xticklabels(labels, rotation=35, ha='right')\n",
        "plt.yticks(fontsize=12)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYzfTq6sWk18"
      },
      "outputs": [],
      "source": [
        "# Selecting columns for plotting\n",
        "selected_columns = ['returns_top_quantile', 'returns_bottom_quantile']\n",
        "\n",
        "labels = ['CURRENT_EV_TO\\nT12M_EBITDA', 'EQY_REC_CONS',\n",
        "       'FIVE_YEAR_AVG\\nEV_TO_T12_EBITDA', 'PE_RATIO', 'RSI_14D', 'RSI_30D',\n",
        "       'RSI_9D']\n",
        "\n",
        "# Plotting the selected data\n",
        "ax = indicator_IR[selected_rows][selected_columns].plot(kind='bar', figsize=(10, 6))\n",
        "\n",
        "Xstart, Xend = ax.get_xlim()\n",
        "Ystart, Yend = ax.get_ylim()\n",
        "\n",
        "ax.text(Xstart+0.5, Yend+0.2,'''Returns Comparison Between Top and Bottom Quantiles of indicators with IR > 0.5''', fontsize=14)\n",
        "\n",
        "plt.xlabel('Indicators', fontsize=14)\n",
        "plt.ylabel('Returns', fontsize=14)\n",
        "plt.legend(['First Quantile', 'Fifth Quantile'], fontsize=12)\n",
        "ax.set_xticklabels(labels, rotation=35, ha='right')\n",
        "plt.yticks(fontsize=12)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtyaxPShWk19"
      },
      "outputs": [],
      "source": [
        "for ind in indicator_IR[selected_rows].index:\n",
        "    result_dfs, portfolio_returns = calculate_quantile_returns(df, ind, returns)\n",
        "\n",
        "    plt.figure(figsize=(15, 6))\n",
        "\n",
        "    # Iterate through quantiles\n",
        "    for quantile in range(len(portfolio_returns.columns)):\n",
        "        plt.plot(portfolio_returns[quantile].cumsum(), label=f\"Quantile {quantile}\")\n",
        "\n",
        "    plt.plot(benchmark_returns.cumsum(), 'k--', label='Benchmark Returns')\n",
        "    plt.title(f\"Cumulative Returns for {ind}\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Cumulative Returns\")\n",
        "\n",
        "    # Set x ticks for every 4 months\n",
        "    plt.xticks(range(0, len(portfolio_returns), 4), portfolio_returns.index[::4], rotation=45)\n",
        "\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhLNImOSWk1-"
      },
      "source": [
        "### Correlation\n",
        "\n",
        "- correlazione e diversificazione\n",
        "\n",
        "- a cosa serve l'analisi di correlation\n",
        "- quali sono i metodi con cui si può effettuare un'analisi di correlation\n",
        "- quale metdodo abbiamo scelto noi\n",
        "- cos'è la correlation di kendall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfmPe-NNWk1-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "top = (\n",
        "    (indicator_IR['IR_top_quantile'] > 0) |\n",
        "    (indicator_IR['IR_bottom_quantile'] > 0)\n",
        ")\n",
        "\n",
        "len(indicator_IR[top].index)"
      ],
      "metadata": {
        "id": "Ea9jFF_w5vFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QNXTY30Wk1_"
      },
      "outputs": [],
      "source": [
        "sns.set_palette('Set2')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mov_avg = ['MOV_AVG_10D', 'MOV_AVG_20D', 'MOV_AVG_30D', 'MOV_AVG_40D', 'MOV_AVG_50D', 'MOV_AVG_5D']\n",
        "cluster_mov_avg = df[indicator_IR[top].index][mov_avg].stack().corr(method='kendall')\n",
        "fig, ax = plt.subplots(figsize=(8,6))\n",
        "sns.heatmap(cluster_mov_avg, vmin=-1, vmax=1, cmap='Blues', annot=True, cbar=True)\n",
        "Xstart, Xend = ax.get_xlim()\n",
        "Ystart, Yend = ax.get_ylim()\n",
        "\n",
        "ax.text(Xstart+0.5, Yend-0.2,\n",
        "\n",
        "        '''Correlation between moving averages''', fontsize=20)\n",
        "\n",
        "x_tick_positions = [0, 1, 2, 3, 4, 5]  # Adjust the positions as needed\n",
        "#ax.set_xticks(x_§tick_positions)\n",
        "\n",
        "ax.set_xticklabels(mov_avg, rotation=35, ha='right')\n",
        "ax.set_yticklabels(mov_avg, rotation=0)\n",
        "\n",
        "#ax.set_xticks([])\n",
        "ax.xaxis.set_ticks_position('none')\n",
        "for s in ['top','right','bottom','left']:\n",
        "    ax.spines[s].set_visible(False)\n",
        "#plt.savefig(\"new_heatmap.png\")\n",
        "#plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zuPa1RnA50lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rsi = ['RSI_9D', 'RSI_14D', 'RSI_30D']\n",
        "cluster_rsi = df[indicator_IR[top].index][rsi].stack().corr(method='kendall')\n",
        "fig, ax = plt.subplots(figsize=(7,5))\n",
        "sns.heatmap(cluster_rsi, vmin=-1, vmax=1, cmap='Blues', annot=True, cbar=True)\n",
        "Xstart, Xend = ax.get_xlim()\n",
        "Ystart, Yend = ax.get_ylim()\n",
        "\n",
        "ax.text(Xstart+0.4, Yend-0.2,\n",
        "\n",
        "        '''Correlation between RSI''', fontsize=20)\n",
        "\n",
        "x_tick_positions = [0, 1, 2]  # Adjust the positions as needed\n",
        "#ax.set_xticks(x_§tick_positions)\n",
        "\n",
        "ax.set_xticklabels(rsi, rotation=35, ha='right')\n",
        "ax.set_yticklabels(rsi, rotation=0)\n",
        "\n",
        "#ax.set_xticks([])\n",
        "ax.xaxis.set_ticks_position('none')\n",
        "for s in ['top','right','bottom','left']:\n",
        "    ax.spines[s].set_visible(False)\n",
        "#plt.savefig(\"new_heatmap.png\")\n",
        "#plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ILMHuery56KN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top = [i for i in indicator_IR[indicator_IR['IR_top_quantile']>0.50].index]\n",
        "top.extend(i for i in indicator_IR[indicator_IR['IR_bottom_quantile']>0.50].index)"
      ],
      "metadata": {
        "id": "hOy0EYBE6MIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qeWA1JnWk1_"
      },
      "outputs": [],
      "source": [
        "corr = df[top].stack().corr(method='kendall')\n",
        "fig, ax = plt.subplots(figsize=(8,6))\n",
        "sns.heatmap(corr, vmin=-1, vmax=1, cmap='Blues', annot=True, cbar=True)\n",
        "Xstart, Xend = ax.get_xlim()\n",
        "Ystart, Yend = ax.get_ylim()\n",
        "\n",
        "ax.text(Xstart+0.2, Yend-0.2, '''Correlation between indicators with IR>0.50''', fontsize=14)\n",
        "\n",
        "x_tick_positions = [0, 1, 2, 3, 4, 5,6]  # Adjust the positions as needed\n",
        "#ax.set_xticks(x_tick_positions)\n",
        "\n",
        "ax.set_xticklabels(labels, rotation=35, ha='right')\n",
        "ax.set_yticklabels(labels, rotation=0)\n",
        "\n",
        "#ax.set_xticks([])\n",
        "ax.xaxis.set_ticks_position('none')\n",
        "for s in ['top','right','bottom','left']:\n",
        "    ax.spines[s].set_visible(False)\n",
        "#plt.savefig(\"new_heatmap.png\")\n",
        "#plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4i6XPmiVFpK"
      },
      "source": [
        "### Turnover\n",
        "\n",
        "We decided to set commission costs to zero, but still we decided to perform a Turnover analysis on the univariate strategies to see which indicators give a greater turnover over the stock selection that would lead to larger costs.\n",
        "\n",
        "\n",
        "- analisi del turnover delle strategie univariate per tutti gli indicatori\n",
        "- come lo abbiamo calcolato (assoluto, relativo, medio)\n",
        "- commento dei risultati e grafici"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ADQ7LvvTz-7"
      },
      "outputs": [],
      "source": [
        "# saving in the dataframes 'first_quantile_stocks' and 'last_quantile_stocks'\n",
        "# all the tickers of the stocks that have been selected for every timestamp\n",
        "\n",
        "first_quantile_stocks = pd.DataFrame(index=df.index, columns=indicators)\n",
        "last_quantile_stocks = pd.DataFrame(index=df.index, columns=indicators)\n",
        "\n",
        "\n",
        "# Loop through each indicator\n",
        "for ind in indicators:\n",
        "    result_dfs, portfolio_returns = calculate_quantile_returns(df, ind, returns)\n",
        "\n",
        "    # For each date, store the list of best stocks in the corresponding column\n",
        "    for index, row in result_dfs[4].iterrows():\n",
        "        first_quantile_stocks.loc[index, ind] = list(row.dropna().index)\n",
        "\n",
        "    for index, row in result_dfs[0].iterrows():\n",
        "        last_quantile_stocks.loc[index, ind] = list(row.dropna().index)\n",
        "\n",
        "\n",
        "first_quantile_date = first_quantile_stocks.reset_index()\n",
        "last_quantile_date = last_quantile_stocks.reset_index()\n",
        "\n",
        "\n",
        "turnover_first_quantile = pd.DataFrame(index = df.index)\n",
        "turnover_last_quantile = pd.DataFrame(index = df.index)\n",
        "\n",
        "lengths_first_quantile = pd.DataFrame(index = df.index)\n",
        "lengths_last_quantile = pd.DataFrame(index = df.index)\n",
        "\n",
        "\n",
        "# this function calculates the turnover by confronting the stocks in the\n",
        "# portfolio at the time t and t-1\n",
        "def turnover_select(indicator, stocks, stocks_date):\n",
        "\n",
        "\n",
        "    expanded_df2 = stocks[indicator].explode()\n",
        "    merged_df = pd.merge(stocks_date[[indicator, 'Date']],\n",
        "                         expanded_df2, how='right',\n",
        "                         left_on='Date', right_on='Date')\n",
        "\n",
        "    changed_stocks = (\n",
        "        merged_df.groupby('Date')[indicator + '_y']\n",
        "        .apply(set)\n",
        "        .diff()\n",
        "        .apply(lambda x: list(x) if isinstance(x, set) else [])\n",
        "    )\n",
        "\n",
        "    lenght_stocks = (\n",
        "      merged_df.groupby('Date')[indicator+ '_y']\n",
        "      .count()\n",
        "    )\n",
        "\n",
        "    if stocks.equals(first_quantile_stocks) and stocks_date.equals(first_quantile_date):\n",
        "      lengths_first_quantile[indicator] = lenght_stocks\n",
        "      turnover_first_quantile[indicator] = changed_stocks.apply(lambda x: len(x))\n",
        "\n",
        "    elif stocks.equals(last_quantile_stocks) and stocks_date.equals(last_quantile_date):\n",
        "      lengths_last_quantile[indicator] = lenght_stocks\n",
        "      turnover_last_quantile[indicator] = changed_stocks.apply(lambda x: len(x))\n",
        "\n",
        "# apply the function for every indicator in the dataset\n",
        "for ind in indicators:\n",
        "\n",
        "    turnover_select(indicator=ind,\n",
        "                    stocks=first_quantile_stocks,\n",
        "                    stocks_date=first_quantile_date)\n",
        "\n",
        "    turnover_select(indicator=ind,\n",
        "                    stocks=last_quantile_stocks,\n",
        "                    stocks_date= last_quantile_date)\n",
        "\n",
        "\n",
        "# we dropped the first two rows of the dataset since there are no stock in the portfolio\n",
        "# in january of 2003 and also the second row becaues every stock is different and\n",
        "# would later affect the calculations of the summary statistics\n",
        "turnover_first_quantile.drop(['2003-01-31', '2003-02-28'], axis=0, inplace=True)\n",
        "turnover_last_quantile.drop(['2003-01-31', '2003-02-28'], axis=0, inplace=True)\n",
        "lengths_first_quantile.drop(['2003-01-31', '2003-02-28'], axis=0, inplace=True)\n",
        "lengths_last_quantile.drop(['2003-01-31', '2003-02-28'], axis=0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZiPB0gVWk2B"
      },
      "outputs": [],
      "source": [
        "labels = ['EQY_REC_CONS', 'RSI_14D', 'RSI_30D', 'RSI_9D',\n",
        "          'CURRENT_EV_TO_T12M_EBITDA', 'FIVE_YEAR_AVG_EV_TO_T12_EBITDA', 'PE_RATIO']\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
        "\n",
        "# Plot and format the first subplot\n",
        "for column, label in zip(top, labels):\n",
        "    ax1.plot(turnover_first_quantile[column].rolling(6).mean(), label=label)\n",
        "ax1.set_title('Turnover - First Quantile - Indicators with IR>0.5')\n",
        "ax1.grid(True, which='both', linestyle='--', linewidth=0.5)  # Add gridlines\n",
        "ax1.legend()  # Add legend\n",
        "\n",
        "# Plot and format the second subplot\n",
        "ax2.plot(turnover_first_quantile[top].rolling(6).mean())\n",
        "ax2.set_title('Turnover - Last Quantile - Indicators with IR>0.5')\n",
        "ax2.grid(True, which='both', linestyle='--', linewidth=0.5)  # Add gridlines\n",
        "\n",
        "xticks_major = ax2.get_xticks()[1::6]  # Adjust the step as needed\n",
        "xtick_labels = turnover_first_quantile.index[xticks_major]\n",
        "ax2.set_xticks(xticks_major)\n",
        "ax2.set_xticklabels(xtick_labels, rotation=45, ha='right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see the indicator that generates the most turnover is 'EQY_REC_CONS' followed by momentum indicators in order of their frequency. The indicator that has the least turnover, from the ones selected is 'FIVE_YEAR_AVG_EV_TO_T12_EBITDA'."
      ],
      "metadata": {
        "id": "JrTTcoJb-UEO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ne5g1oRrTz-8"
      },
      "outputs": [],
      "source": [
        "# We are calculating some summary statistics of the turnver for each indicator\n",
        "\n",
        "rel_first_quantile = round(turnover_first_quantile / lengths_first_quantile,2) * 100\n",
        "rel_last_quantile = round(turnover_last_quantile / lengths_last_quantile,2) * 100\n",
        "\n",
        "stats = round(turnover_first_quantile.mean(),2).reset_index()\n",
        "stats.columns = ['indicators', 'average_first_quantile']\n",
        "\n",
        "stats2 = round(turnover_last_quantile.mean(),2).reset_index()\n",
        "stats2.columns = ['indicators', 'average_last_quantile']\n",
        "\n",
        "stats3 = round(rel_first_quantile.mean(),2).reset_index()\n",
        "stats3.columns = ['indicators', 'average_perc_first_quantile']\n",
        "\n",
        "stats4 = round(rel_last_quantile.mean(),2).reset_index()\n",
        "stats4.columns = ['indicators', 'average__perc_last_quantile']\n",
        "\n",
        "stats3 = stats3.merge(stats4, how='inner', on='indicators')\n",
        "stats = stats.merge(stats2, how='inner', on='indicators')\n",
        "\n",
        "stats = stats.merge(stats3, how='inner', on='indicators')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqxl6uWPu-lJ"
      },
      "outputs": [],
      "source": [
        "stats.sort_values(by='average_first_quantile', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We divided indicators into three main categories:\n",
        "1. value measures\n",
        "2. profitability measures\n",
        "3. momentum based measures\n",
        "\n",
        "This will give us more insight into the turnover behaviour but it will be useful later for the multivariate strategy"
      ],
      "metadata": {
        "id": "UJKU0-Yp_8or"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indicator_categories = {\n",
        "    '5YR_AVG_RETURN_ON_EQUITY': 'Profitability',\n",
        "    'CURRENT_EV_TO_12M_SALES': 'Value',\n",
        "    'CURRENT_EV_TO_T12M_EBITDA': 'Value',\n",
        "    'CUR_MKT_CAP': 'Value',\n",
        "    'EBITDA_MARGIN': 'Profitability',\n",
        "    'EBITDA_MARGIN_3YR_AVG': 'Profitability',\n",
        "    'EQY_REC_CONS': 'Momentum',\n",
        "    'FIVE_YEAR_AVG_EV_TO_T12_EBITDA': 'Value',\n",
        "    'FIVE_YR_AVG_PRICE_EARNINGS': 'Value',\n",
        "    'MOV_AVG_10D': 'Momentum',\n",
        "    'MOV_AVG_20D': 'Momentum',\n",
        "    'MOV_AVG_30D': 'Momentum',\n",
        "    'MOV_AVG_40D': 'Momentum',\n",
        "    'MOV_AVG_50D': 'Momentum',\n",
        "    'MOV_AVG_5D': 'Momentum',\n",
        "    'NET_DEBT_PER_SHARE': 'Profitability',\n",
        "    'NORMALIZED_ACCRUALS_BS_METHOD': 'Profitability',\n",
        "    'NORMALIZED_ACCRUALS_CF_METHOD': 'Profitability',\n",
        "    'NORMALIZED_ROE': 'Profitability',\n",
        "    'OPERATING_ROIC': 'Profitability',\n",
        "    'PE_RATIO': 'Value',\n",
        "    'PX_LAST': np.nan,  # This one doesn't fall neatly into a single category\n",
        "    'PX_TO_BOOK_RATIO': 'Value',\n",
        "    'PX_TO_TANG_BV_PER_SH': 'Value',\n",
        "    'RSI_14D': 'Momentum',\n",
        "    'RSI_30D': 'Momentum',\n",
        "    'RSI_9D': 'Momentum',\n",
        "    'TANG_BOOK_VAL_PER_SH': 'Profitability',\n",
        "    'TRAIL_12M_EBITDA_PER_SHARE': 'Value',\n",
        "    'TRAIL_12M_SALES_PER_SH': 'Value',\n",
        "    'VOLATILITY_180D': 'Profitability',\n",
        "    'VOLATILITY_30D': 'Profitability',\n",
        "    'VOLATILITY_90D': 'Profitability',\n",
        "    'WACC_COST_EQUITY': 'Profitability'\n",
        "}\n",
        "\n",
        "stats['Category'] = stats['indicators'].map(indicator_categories)"
      ],
      "metadata": {
        "id": "c9z8msAg6h6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats.groupby('Category')[['average_first_quantile','average_last_quantile','average_perc_first_quantile','average__perc_last_quantile']].mean()"
      ],
      "metadata": {
        "id": "amPPfzNZ6kkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall momentum and profitability measures lead to an higher turnover in the stock selection process rather than univariate Value strategies."
      ],
      "metadata": {
        "id": "-nxe2aJhAev8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D1v1mihTz-9"
      },
      "source": [
        "## Multivariate strategy  \n",
        "\n",
        "Multivariate stock ranking with the Z-score, unlike the univariate one, involves the simultaneous consideration of multiple financial metrics to assess and rank the performance of stocks within a given universe. This methodology integrates statistical analysis, particularly the Z-score, to create a composite indicator that aids investors in making more informed decisions based on a holistic evaluation of a company's financial health.\n",
        "\n",
        "In this approach, the Z-score, a statistical measure standardizing and quantifying a data point's deviation from the mean within a dataset, is applied to various financial ratios and indicators. This facilitates the comparison of stocks across different criteria.\n",
        "\n",
        "Multivariate stock ranking extends beyond traditional univariate approaches by incorporating a diverse set of financial metrics. The amalgamation of these factors provides a more comprehensive assessment of a stock's overall performance.\n",
        "\n",
        "First of all, we calculated the single Z-scores for every indicator, normalizing all the values of the dataset. After that, we calculated the composite Z-score for every stock, by summing the single Z-scores of the selected indicators, using the performance obtained in the univariate strategy to choose the sign of the influence of the particular indicator on the overall Z-score.\n",
        "We used different methods to select the indicators to include. The first method considers all the indicators that performed best in the Univariate strategy: we selected all the indicators that had an IR greater than 0.5 in the top or bottom quantiles, using the negative values for computing the Z-score for the bottom quantiles. This is a procedure that only takes into account the univariate results, without considering the nature of the indicators selected. For this reason, many indicators representing similar features can be selected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFi9CrmaTz-9"
      },
      "outputs": [],
      "source": [
        "def calculate_quantile_returns_zscore(data_frame, column_name, returns_df, num_quantiles=5):\n",
        "    \"\"\"\n",
        "    Calculate quantile returns based on the specified column in the DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    quantile_ranks = pd.DataFrame(index=data_frame.index, columns=data_frame.columns)\n",
        "    ind_rank = data_frame[column_name]\n",
        "    ind_rank = ind_rank.replace(0, np.nan)\n",
        "\n",
        "    for date in ind_rank.index:\n",
        "        row_values = ind_rank.loc[date]\n",
        "        if row_values.count() > 1:\n",
        "          ranks = pd.Series(row_values).rank(method='max')\n",
        "          quintiles = pd.qcut(ranks, q=num_quantiles, labels=False)\n",
        "          quantile_ranks.loc[date] = quintiles\n",
        "\n",
        "    quantile_dfs = {}\n",
        "    portfolio_returns = pd.DataFrame()\n",
        "\n",
        "    for quantile in range(num_quantiles):\n",
        "        filtered_df = returns_df[quantile_ranks == quantile]\n",
        "        filtered_df_shifted = filtered_df\n",
        "        quantile_dfs[quantile] = filtered_df_shifted\n",
        "        portfolio_returns[quantile] = quantile_dfs[quantile].mean(axis=1).dropna()\n",
        "\n",
        "    return quantile_dfs, portfolio_returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlm9FF3FTz--"
      },
      "outputs": [],
      "source": [
        "z_score = (df-df.T.groupby(level=0).mean().T)/df.T.groupby(level=0).std().T\n",
        "\n",
        "levels = [0.1, 0.25, 0.5, 1, 1.5, 2]\n",
        "indicator_IR_z = pd.DataFrame(index=levels, columns=['IR_max_quant', 'tracking_error_max', 'returns_max'])\n",
        "\n",
        "for selection_level in levels:\n",
        "    sel_z_score = z_score[indicator_IR[(indicator_IR['IR_top_quantile']>selection_level)].index].merge(-z_score[indicator_IR[(indicator_IR['IR_bottom_quantile']>selection_level)].index], on='Date')\n",
        "    final = sel_z_score.T.groupby(level=1).sum().T\n",
        "\n",
        "    result_dfs_2, portfolio_returns_2 = calculate_quantile_returns_zscore(final, final.columns, returns)\n",
        "\n",
        "    plt.figure(figsize=(15, 6))\n",
        "\n",
        "    # Iterate through quantiles\n",
        "    for quantile in range(len(portfolio_returns_2.columns)):\n",
        "        plt.plot(portfolio_returns_2[quantile].cumsum(), label=f\"Quantile {quantile}\")\n",
        "\n",
        "    plt.title(f\"Cumulative Returns for IR > {selection_level}\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Cumulative Returns\")\n",
        "\n",
        "    # Set x ticks for every 4 months\n",
        "    plt.xticks(range(0, len(portfolio_returns_2), 4), portfolio_returns_2.index[::4], rotation=45)\n",
        "\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    indicator_IR_z.at[selection_level, 'IR_max_quant'] = calculate_IR(portfolio_returns_2[4], benchmark_returns)\n",
        "    indicator_IR_z.at[selection_level, 'tracking_error_max'] = np.std(portfolio_returns_2[4]-benchmark_returns)\n",
        "    indicator_IR_z.at[selection_level, 'returns_max'] = np.mean(portfolio_returns_2[4]) * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graphs show the returns changing the level of IR treshold used to select the factors to compute the composite Z-score."
      ],
      "metadata": {
        "id": "uuHZGr9bNaDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indicator_IR_z"
      ],
      "metadata": {
        "id": "pv8qMdLPQVKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This strategy, while mantaining the returns almost as good as the best univariate factors, is able to reduce tracking errors, thus mantaining excellent IRs.\n",
        "\n",
        "As a second strategy, we computed a composite Z-score using factors that could describe different features of the stocks: we selected one momentum indicator, two value indicators and one profitability indicator, taking into account univariate performance and checking the correlation between the factors."
      ],
      "metadata": {
        "id": "l54ek9y5QZsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indicator_IR_z_2 = pd.DataFrame(columns=['IR_max_quant', 'tracking_error_max', 'returns_max'])\n",
        "\n",
        "pos_factors = [\n",
        "    \"RSI_9D\",\n",
        "]\n",
        "\n",
        "neg_factors = [\n",
        "    \"PE_RATIO\",\n",
        "    \"PX_TO_BOOK_RATIO\",\n",
        "    \"NORMALIZED_ACCRUALS_CF_METHOD\"\n",
        "]\n",
        "\n",
        "sel_z_score = z_score[pos_factors].merge(-z_score[neg_factors], on='Date')\n",
        "final = sel_z_score.T.groupby(level=1).sum().T\n",
        "\n",
        "corr = sel_z_score.stack().corr(method='kendall')\n",
        "fig, ax = plt.subplots(figsize=(8,6))\n",
        "sns.heatmap(corr, vmin=-1, vmax=1, cmap='Blues', annot=True, cbar=True)\n",
        "Xstart, Xend = ax.get_xlim()\n",
        "Ystart, Yend = ax.get_ylim()\n",
        "\n",
        "ax.text(Xstart+1.4, Yend-0.2,\n",
        "\n",
        "        '''Correlation between chosen\n",
        "                indicators''', fontsize=20)\n",
        "\n",
        "x_tick_positions = [0, 1, 2, 3, 4, 5,6]  # Adjust the positions as needed\n",
        "#ax.set_xticks(x_tick_positions)\n",
        "\n",
        "# ax.set_xticklabels(labels, rotation=35, ha='right')\n",
        "# ax.set_yticklabels(labels, rotation=0)\n",
        "\n",
        "#ax.set_xticks([])\n",
        "ax.xaxis.set_ticks_position('none')\n",
        "for s in ['top','right','bottom','left']:\n",
        "    ax.spines[s].set_visible(False)\n",
        "#plt.savefig(\"new_heatmap.png\")\n",
        "#plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kV5LC-SycKKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4Yx69opTz--"
      },
      "outputs": [],
      "source": [
        "result_dfs_3, portfolio_returns_3 = calculate_quantile_returns_zscore(final, final.columns, returns)\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# Iterate through quantiles\n",
        "for quantile in range(len(portfolio_returns_3.columns)):\n",
        "    plt.plot(portfolio_returns_3[quantile].cumsum(), label=f\"Quantile {quantile}\")\n",
        "\n",
        "plt.title(f\"Cumulative Returns\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Cumulative Returns\")\n",
        "\n",
        "# Set x ticks for every 4 months\n",
        "plt.xticks(range(0, len(portfolio_returns_3), 4), portfolio_returns_3.index[::4], rotation=45)\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "indicator_IR_z_2.at[0, 'IR_max_quant'] = calculate_IR(portfolio_returns_3[4], benchmark_returns)\n",
        "indicator_IR_z_2.at[0, 'tracking_error_max'] = np.std(portfolio_returns_3[4]-benchmark_returns)\n",
        "indicator_IR_z_2.at[0, 'returns_max'] = np.mean(portfolio_returns_3[4]) * 100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indicator_IR_z_2"
      ],
      "metadata": {
        "id": "BqNeZbasQc_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the result of this method outperform the results of the multivariate method proposed before: the return obtained is slighty inferior but the Information Ratio and the Tracking Error show better values."
      ],
      "metadata": {
        "id": "ZzDIlW-kdzT9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0_fNYVDWk2L"
      },
      "source": [
        "## Machine Learning to explain returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7iHe-A9Wk2M"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eURekajBWk2M"
      },
      "source": [
        "We implement a Random Forest Regressor that tries to explain the returns of the equity for the following month given the input factors used in the previous models. We compare the metrics of the regressor with a Linear Regression model to have a benchmark of the results obtained.\n",
        "\n",
        "We choose to use a Random Forest because is well known to be one of the best performer models. Furthermore, it is very versatile and less computationally expensive than other machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQU0Z-QNWk2M"
      },
      "outputs": [],
      "source": [
        "subset = data_stack.loc['2003-03-31':'2004-02-27']\n",
        "\n",
        "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imputer = KNNImputer(n_neighbors=2)\n",
        "\n",
        "subset_mean = imp_mean.fit_transform(subset)\n",
        "subset_knn = imputer.fit_transform(subset)\n",
        "\n",
        "subset = subset.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1FHBBG1Wk2N"
      },
      "outputs": [],
      "source": [
        "num_columns = subset.shape[1]\n",
        "fig, axs = plt.subplots(num_columns, figsize=(10, 4*num_columns))\n",
        "\n",
        "# Plot the distribution of each array in each subplot\n",
        "for i in range(num_columns):\n",
        "    axs[i].hist(subset[:, i], alpha=0.5, label='Subset', bins=20)\n",
        "    axs[i].hist(subset_mean[:, i], alpha=0.5, label='Subset Mean', bins=20)\n",
        "    axs[i].hist(subset_knn[:, i], alpha=0.5, label='Subset KNN', bins=20)\n",
        "    axs[i].set_title(f'Column {i+1}')\n",
        "    axs[i].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13RaNrdmWk2N"
      },
      "source": [
        "### Random Forest\n",
        "\n",
        "We implement a Random Forest Regressor through the Scikit Learn library. We use a rolling window to train and test the model, simulating a real time situation where the user knows the indicators and returns for the last months and tries to predict the returns for the following month given the indicators of the present month. The rolling window is 12 month wide, and is shifted by one month at every iteration. At each iteration a model is trained on the 12 month rolling window and tries to use the following month data to predict the returns. Then, the rolling window is shifted by one month and the process is repeated.\n",
        "\n",
        "Finally, regression metrics are computing (MAE, MSE and RMSE), confronting the predictions with the actual returns of the stocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-Q2kk1BWk2N"
      },
      "outputs": [],
      "source": [
        "def xs_forest(df, n_estimators=4):\n",
        "    '''Build a cross-sectional forest model and\n",
        "    output the pipeline at each timestamp'''\n",
        "    # sklearn does not have an imputing method unless specified\n",
        "    df = df[df['RETURNS'].notna()]\n",
        "    # do not leave out test sample in our training pipeline\n",
        "    X_train, y_train = df.shift(1).drop(columns='RETURNS').values, df['RETURNS'].values\n",
        "\n",
        "    model = make_pipeline(KNNImputer(n_neighbors=3),\n",
        "                          RandomForestRegressor(n_estimators=n_estimators,\n",
        "                                                random_state=42))\n",
        "    model.fit(X_train, y_train)\n",
        "    # we return the model instead of slope estimations\n",
        "    # such that we can transform the next-period data later\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHqS0VzqWk2O"
      },
      "outputs": [],
      "source": [
        "ls_forest = {}\n",
        "ts = df.index.to_list()\n",
        "dt = 12                                # 1-year rolling window\n",
        "for i in range(len(ts)-dt+1):\n",
        "    df3 = data_stack.loc[ts[i]:ts[i+dt-1]]\n",
        "    ls_forest[ts[i+dt-1]] = xs_forest(df3)    # add the model corresponding to i+dt-1, last period of rolling window\n",
        "df_forest = pd.Series(ls_forest)\n",
        "\n",
        "res = {}\n",
        "\n",
        "for t in df_forest.shift(1).index.to_list():\n",
        "    df4 = data_stack.loc[t]\n",
        "    df4 = df4[df4['RETURNS'].notna()]\n",
        "    X_test, y_test = df4.shift(1).drop(columns='RETURNS').values, df4['RETURNS'].values\n",
        "    try:\n",
        "        forest = df_forest.shift(1).loc[t]\n",
        "        y_pred = forest.predict(X_test).reshape(-1)  # prediction array reshaped to 1d\n",
        "    except:\n",
        "        y_pred = np.nan\n",
        "    res[t] = pd.DataFrame({'RETURNS': y_test, 'PREDICTIONS': y_pred}, index=df4.index)\n",
        "\n",
        "res = pd.concat(res)\n",
        "res.index.names = ['Date', 'ticker']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuLfEk00Wk2O"
      },
      "outputs": [],
      "source": [
        "feature_importance = pd.DataFrame()\n",
        "for i in range(len(ts)-dt+1):\n",
        "  feature_importance[ts[i+dt-1]] = ls_forest[ts[i+dt-1]][1].feature_importances_\n",
        "impo_means = feature_importance.mean(axis = 1)\n",
        "features = pd.Series(impo_means.values, data_stack.drop('RETURNS', axis=1).columns)\n",
        "features.sort_values(ascending=False).plot.bar(fontsize=7, title='Feature Importance expressed as Gini impurity (average on all models trained)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSgC4-M7Wk2O"
      },
      "source": [
        "Comparing the most important features selected by the model with the most important features in Univariate Strategy, we can see that the Relative Strength Index calculated on 9 days and the Price to Earnings Ratio are present in both groups. The machine learning model, furthermore, selects as important features the Volatily computed on different periods, the Price to Tangible Book Value per Share and the Current Market Capitalization, that did not outperform in the Univariate Strategy.\n",
        "\n",
        "This shows that the model can find different patterns than the simple univariate strategy, that allow to predict the future return."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nKBFm9eWk2O"
      },
      "outputs": [],
      "source": [
        "df_ret = res.drop('2003-12-31', level=0, axis=0)['RETURNS']\n",
        "df_pred = res.drop('2003-12-31', level=0, axis=0)['PREDICTIONS']\n",
        "print('Root Mean Squared Error (RMSE):', round(metrics.mean_squared_error(df_ret, df_pred, squared=False),ndigits=3))\n",
        "print('Mean Absolute Error (MAE):', round(metrics.mean_absolute_error(df_ret, df_pred),ndigits=3))\n",
        "print('Mean Squared Error (MSE):', round(metrics.mean_squared_error(df_ret, df_pred),ndigits=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qigESIAWk2P"
      },
      "source": [
        "The output metrics are good, showing a good ability of the model to predict the returns.\n",
        "\n",
        "### Linear Regression\n",
        "\n",
        "We apply a Linear Regression using the same method described before, to have a benchmark for the results obtained by the Random Forest Regressor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14sdvZMEWk2P"
      },
      "outputs": [],
      "source": [
        "def xs_lr(df, n_estimators=4):\n",
        "    '''Build a cross-sectional linear regression model and\n",
        "    output the pipeline at each timestamp'''\n",
        "    # sklearn does not have an imputing method unless specified\n",
        "    df = df[df['RETURNS'].notna()]\n",
        "    # do not leave out test sample in our training pipeline\n",
        "    X_train, y_train = df.shift(1).drop(columns='RETURNS').values, df['RETURNS'].values\n",
        "\n",
        "    model = make_pipeline(KNNImputer(n_neighbors=3),\n",
        "                          LinearRegression())\n",
        "    model.fit(X_train, y_train)\n",
        "    # we return the model instead of slope estimations\n",
        "    # such that we can transform the next-period data later\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-mtg3vUWk2P"
      },
      "outputs": [],
      "source": [
        "ls_lr = {}\n",
        "ts = df.index.to_list()\n",
        "dt = 12                                # 1-year rolling window\n",
        "for i in range(len(ts)-dt+1):\n",
        "    df3 = data_stack.loc[ts[i]:ts[i+dt-1]]\n",
        "    ls_lr[ts[i+dt-1]] = xs_lr(df3)    # add the model corresponding to i+dt-1, last period of rolling window\n",
        "df_lr = pd.Series(ls_lr)\n",
        "\n",
        "res_lr = {}\n",
        "\n",
        "for t in df_lr.shift(1).index.to_list():\n",
        "    df4 = data_stack.loc[t]\n",
        "    df4 = df4[df4['RETURNS'].notna()]\n",
        "    X_test, y_test = df4.shift(1).drop(columns='RETURNS').values, df4['RETURNS'].values\n",
        "    try:\n",
        "        lr = df_lr.shift(1).loc[t]\n",
        "        y_pred = lr.predict(X_test).reshape(-1)  # prediction array reshaped to 1d\n",
        "    except:\n",
        "        y_pred = np.nan\n",
        "    res_lr[t] = pd.DataFrame({'RETURNS': y_test, 'PREDICTIONS': y_pred}, index=df4.index)\n",
        "\n",
        "res_lr = pd.concat(res_lr)\n",
        "res_lr.index.names = ['Date', 'ticker']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzcKB8g-Wk2Q"
      },
      "outputs": [],
      "source": [
        "feature_importance_lr = pd.DataFrame()\n",
        "for i in range(len(ts)-dt+1):\n",
        "  feature_importance_lr[ts[i+dt-1]] = abs(ls_lr[ts[i+dt-1]][1].coef_)\n",
        "impo_means_lr = feature_importance_lr.mean(axis = 1)\n",
        "features_lr = pd.Series(impo_means_lr.values, data_stack.drop('RETURNS', axis=1).columns)\n",
        "features_lr.sort_values(ascending=False).plot.bar(fontsize=7, title='Feature Importance expressed as absolute value of coefficient (average on all models trained)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DV7Gn07fWk2Q"
      },
      "source": [
        "We can see that some important parameters selected by the Linear Regression are similar to the one selected by Univariate strategy: taking into consideration the seven most important features, both groups include Relative Strenght Index combuted at 9, 14 or 30 days, and Consesus Equity Recommendations. The Linear Regression, furthermore, selects WACC - Cost Equity and Normalized Accruals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLVq4g3jWk2Q"
      },
      "outputs": [],
      "source": [
        "df_ret_lr = res_lr.drop('2003-12-31', level=0, axis=0)['RETURNS']\n",
        "df_pred_lr = res_lr.drop('2003-12-31', level=0, axis=0)['PREDICTIONS']\n",
        "\n",
        "table = [['Root Mean Squared Error (RMSE):',\n",
        "          round(metrics.mean_squared_error(df_ret_lr, df_pred_lr, squared=False),ndigits=3),\n",
        "          round(metrics.mean_squared_error(df_ret, df_pred, squared=False),ndigits=3)\n",
        "          ],\n",
        "         ['Mean Absolute Error (MAE):',\n",
        "          round(metrics.mean_absolute_error(df_ret_lr, df_pred_lr),ndigits=3),\n",
        "          round(metrics.mean_absolute_error(df_ret, df_pred),ndigits=3)\n",
        "          ],\n",
        "         ['Mean Squared Error (MSE):',\n",
        "          round(metrics.mean_squared_error(df_ret_lr, df_pred_lr),ndigits=3),\n",
        "          round(metrics.mean_squared_error(df_ret, df_pred),ndigits=3)\n",
        "          ]]\n",
        "\n",
        "print(tabulate(table, headers=['','LR', 'RF']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TN0yktWtWk2Q"
      },
      "source": [
        "Comparing the metrics with the ones obtained with the Random Forest Regression, we can see that the ability of the Random Forest to predict the earning is way higher: its errors are, in fact, lower. Since Linear Regression is a very simple model, we expected this behaviour.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}